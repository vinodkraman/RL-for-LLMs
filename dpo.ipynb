{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DPO from Scratch"
      ],
      "metadata": {
        "id": "T0NQE2aiWPDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from https://arxiv.org/abs/2305.18290"
      ],
      "metadata": {
        "id": "wTfS16KXB1F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "import wandb\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "VU_EbAjVoeq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=2003):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "COemx990prmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dpo_loss(model_pref_logp, model_dispref_logp, ref_pref_logp, ref_dispref_logp, beta=0.50):\n",
        "  diff_pref_logp = model_pref_logp - ref_pref_logp\n",
        "  diff_dispref_logp = model_dispref_logp - ref_dispref_logp\n",
        "  loss = -F.logsigmoid(beta * (diff_pref_logp - diff_dispref_logp)).mean()\n",
        "  reward_margins = (diff_pref_logp - diff_dispref_logp).mean()\n",
        "  return loss, diff_pref_logp.mean(), diff_dispref_logp.mean(), reward_margins\n"
      ],
      "metadata": {
        "id": "pbMZXPjv1N95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_logp(policy, token_ids, attention_mask, prompt_lengths):\n",
        "  logits = policy(\n",
        "          input_ids=token_ids,\n",
        "          attention_mask=attention_mask\n",
        "      ).logits #shape (batch_size, sequence_length, vocab_size)\n",
        "\n",
        "  log_probs = F.log_softmax(logits, dim=-1)\n",
        "  token_log_probs = torch.gather(log_probs, -1, token_ids.unsqueeze(-1)).squeeze(-1)\n",
        "  batch_size, seq_len = token_ids.shape\n",
        "  response_mask = torch.arange(seq_len, device=token_ids.device).unsqueeze(0) >= prompt_lengths.unsqueeze(1)\n",
        "  response_mask = response_mask.float()\n",
        "  response_sum_log_probs = (token_log_probs * response_mask).sum(dim=-1)\n",
        "  response_lengths = response_mask.sum(dim=-1).clamp(min=1)\n",
        "  return response_sum_log_probs / response_lengths"
      ],
      "metadata": {
        "id": "mDXZPGaY1__K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for DPO dataset.\n",
        "    Returns raw strings only (no tokenization here).\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"prompts\": ['Instruct: ' + item['prompt'] + '\\n' for item in batch],\n",
        "        \"chosen\": ['Output: ' + item['chosen'] for item in batch],\n",
        "        \"rejected\": ['Output: ' + item['rejected'] for item in batch],\n",
        "    }\n"
      ],
      "metadata": {
        "id": "2876p12p2XP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DPOTrainer:\n",
        "  def __init__(self,\n",
        "                dataset_name=\"HuggingFaceH4/ultrachat_200k\",\n",
        "                split=\"train\",\n",
        "                policy_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                ref_name=None,\n",
        "                reward_name=\"OpenAssistant/reward-model-deberta-v3-base\",\n",
        "                output_dir=\"./dpo-policy\",\n",
        "                epochs=1, batch_size=2,\n",
        "                max_length= 128,\n",
        "                lr=1e-5,\n",
        "                beta=0.2,\n",
        "                seed=42, device=None):\n",
        "      set_seed(seed)\n",
        "      self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.epochs = epochs\n",
        "      self.beta = beta\n",
        "      self.max_length = max_length\n",
        "      self.output_dir = output_dir\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(policy_name, padding_side=\"left\", use_fast=True, model_max_length=512)\n",
        "      if self.tokenizer.pad_token is None:\n",
        "          self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "      self.policy = AutoModelForCausalLM.from_pretrained(policy_name).to(self.device)\n",
        "\n",
        "      self.ref = AutoModelForCausalLM.from_pretrained(ref_name).to(self.device)\n",
        "      for p in self.ref.parameters():\n",
        "          p.requires_grad_(False)\n",
        "\n",
        "      self.opt = torch.optim.AdamW(self.policy.parameters(), lr=lr)\n",
        "\n",
        "      self.ds = load_dataset(dataset_name, split=split)\n",
        "      self.dl = DataLoader(self.ds, batch_size=batch_size, shuffle=True,\n",
        "                            collate_fn=collate_fn)\n",
        "\n",
        "  #performs a single iteration of DPO\n",
        "  def step(self, batch):\n",
        "    prompts = batch[\"prompts\"]\n",
        "    pref_responses = batch[\"chosen\"]\n",
        "    dispref_responses = batch[\"rejected\"]\n",
        "\n",
        "    #tokenize prompts\n",
        "    prompt_encodings = self.tokenizer(\n",
        "        prompts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=self.max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    prompt_lengths = prompt_encodings.attention_mask.sum(dim=-1).to(device)\n",
        "\n",
        "    #tokenize chosen reponses\n",
        "    chosen_encodings = self.tokenizer(\n",
        "        pref_responses,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=self.max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    prompt_preferred_ids = torch.cat([\n",
        "        prompt_encodings.input_ids,\n",
        "        chosen_encodings.input_ids\n",
        "    ], dim=-1).to(self.device)\n",
        "\n",
        "\n",
        "    prompt_preferred_mask = torch.cat([\n",
        "        prompt_encodings.attention_mask,\n",
        "        chosen_encodings.attention_mask\n",
        "    ], dim=-1).to(self.device)\n",
        "\n",
        "    #tokenize rejected responses\n",
        "     rejected_encodings = self.tokenizer(\n",
        "        dispref_responses,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=self.max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    prompt_dispreferred_ids = torch.cat([\n",
        "        prompt_encodings.input_ids,\n",
        "        rejected_encodings.input_ids\n",
        "    ], dim=-1).to(self.device)\n",
        "\n",
        "\n",
        "    prompt_dispreferred_mask = torch.cat([\n",
        "        prompt_encodings.attention_mask,\n",
        "        rejected_encodings.attention_mask\n",
        "    ], dim=-1).to(self.device)\n",
        "\n",
        "\n",
        "    #get model log prob for preferred and dispreferred responses\n",
        "    policy_pref_logp = get_logp(self.policy, prompt_preferred_ids, prompt_preferred_mask, prompt_lengths)\n",
        "    policy_dispref_logp = get_logp(self.policy, prompt_dispreferred_ids, prompt_dispreferred_mask, prompt_lengths)\n",
        "\n",
        "    #get reference log prob for preferred and dispreferred responses\n",
        "    with torch.no_grad():\n",
        "      ref_pref_logp = get_logp(self.ref, prompt_preferred_ids, prompt_preferred_mask, prompt_lengths)\n",
        "      ref_dispref_logp = get_logp(self.ref, prompt_dispreferred_ids, prompt_dispreferred_mask, prompt_lengths)\n",
        "\n",
        "    #compute loss\n",
        "    loss, _, _, _ = dpo_loss(\n",
        "                policy_pref_logp,\n",
        "                policy_dispref_logp,\n",
        "                ref_pref_logp,\n",
        "                ref_dispref_logp,\n",
        "                beta=self.beta\n",
        "            )\n",
        "    return {'loss': loss}\n",
        "\n",
        "  def train(self):\n",
        "    self.policy.train()\n",
        "    self.ref.eval()\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "        for batch in self.dl:\n",
        "            stats = self.step(batch)\n",
        "            step += 1\n",
        "            if step % 5 == 0:\n",
        "                print(\n",
        "                    f\"epoch {epoch} step {step} | \"\n",
        "                    f\"loss {stats['loss']:.4f} | \"\n",
        "                )\n",
        "\n",
        "    # Save final model\n",
        "    if self.output_dir:\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        self.policy.save_pretrained(self.output_dir)\n",
        "        self.tokenizer.save_pretrained(self.output_dir)"
      ],
      "metadata": {
        "id": "-LMrGKYJ4QAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = DPOTrainer(\n",
        "    dataset_name=\"jondurbin/truthy-dpo-v0.1\",\n",
        "    split=\"train\",\n",
        "    policy_name=\"EleutherAI/pythia-70m\",\n",
        "    ref_name=\"EleutherAI/pythia-70m\",\n",
        "    epochs=10, batch_size=8,\n",
        "    max_length=128,\n",
        "    lr=1e-6,\n",
        "    beta=0.10,\n",
        ")\n",
        "trainer.train()\n",
        "print('Saved to', trainer.output_dir)"
      ],
      "metadata": {
        "id": "LwRXolcyA04F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}