{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0AhUxLL9H4CLCmqtV3kEc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinodkraman/RL4LLMs/blob/main/grpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRPO from Scratch"
      ],
      "metadata": {
        "id": "I5t-r0f2WQzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, os, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  AutoModelForSequenceClassification,\n",
        ")\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "  import numpy as np\n",
        "  random.seed(seed); np.random.seed(seed)\n",
        "  torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "@dataclass\n",
        "class Batch:\n",
        "  prompts: List[str]\n",
        "\n",
        "def collate_prompts(features, prompt_column: str):\n",
        "  return Batch(prompts=[f[prompt_column] for f in features])\n",
        "\n",
        "#computes rewards for a batch of responses to prompts\n",
        "def reward_score(rm, rm_tok, prompt: str, response: str, max_length: int, device: torch.device) -> float:\n",
        "  text = prompt.strip() + \"\\n\\n\" + response.strip()\n",
        "  toks = rm_tok(text, return_tensors=\"pt\", truncation=True, max_length=max_length, add_special_tokens=True).to(device)\n",
        "  with torch.no_grad():\n",
        "      out = rm(**toks).logits\n",
        "      if out.size(-1) == 1:\n",
        "          return float(out.squeeze())\n",
        "      return float(out.squeeze(0).mean(dim=-1))\n",
        "\n",
        "#generates responses to prompts\n",
        "def generate_with_scores(model, tokenizer, prompts: List[str], max_new_tokens: int, temperature: float, top_p: float, device: torch.device):\n",
        "  enc = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, add_special_tokens=True).to(device)\n",
        "  with torch.no_grad():\n",
        "      out = model.generate(\n",
        "          **enc,\n",
        "          max_new_tokens=max_new_tokens,\n",
        "          do_sample=True,\n",
        "          temperature=temperature,\n",
        "          top_p=top_p,\n",
        "          return_dict_in_generate=True,\n",
        "          output_scores=True,\n",
        "          pad_token_id=tokenizer.eos_token_id,\n",
        "      )\n",
        "  prompt_lens = enc[\"attention_mask\"].sum(dim=1).tolist()\n",
        "  sequences = out.sequences\n",
        "  input_len = enc[\"input_ids\"].shape[1]\n",
        "  texts = []\n",
        "  for i in range(sequences.size(0)):\n",
        "      cont = sequences[i, input_len:]\n",
        "      texts.append(tokenizer.decode(cont, skip_special_tokens=True))\n",
        "  return out, texts, input_len, prompt_lens\n"
      ],
      "metadata": {
        "id": "5DSJ5jxOXscN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRPOTrainer:\n",
        "  def __init__(self,\n",
        "                dataset_name=\"HuggingFaceH4/ultrachat_200k\",\n",
        "                split=\"train[:1024]\",\n",
        "                prompt_column=\"text\",\n",
        "                policy_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                ref_name=None,\n",
        "                reward_name=\"OpenAssistant/reward-model-deberta-v3-base\",\n",
        "                output_dir=\"./grpo-policy\",\n",
        "                epochs=1, batch_size=2, group_size=4, mu=3,\n",
        "                gen_max_new_tokens=64, rm_max_length=512,\n",
        "                temperature=1.0, top_p=0.95, lr=1e-5,\n",
        "                clip_ratio=0.2, kl_coef=0.01, max_grad_norm=1.0,\n",
        "                seed=42, device=None):\n",
        "      set_seed(seed)\n",
        "      self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.epochs = epochs\n",
        "      self.group_size = group_size\n",
        "      self.gen_max_new_tokens = gen_max_new_tokens\n",
        "      self.rm_max_length = rm_max_length\n",
        "      self.temperature = temperature\n",
        "      self.top_p = top_p\n",
        "      self.mu = mu\n",
        "      self.clip_ratio = clip_ratio\n",
        "      self.kl_coef = kl_coef\n",
        "      self.max_grad_norm = max_grad_norm\n",
        "      self.output_dir = output_dir\n",
        "\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(policy_name, padding_side=\"left\", use_fast=True, model_max_length=512)\n",
        "      if self.tokenizer.pad_token is None:\n",
        "          self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "      self.policy = AutoModelForCausalLM.from_pretrained(policy_name).to(self.device)\n",
        "      self.policy_old = AutoModelForCausalLM.from_pretrained(policy_name).to(self.device)\n",
        "      for p in self.policy_old.parameters():\n",
        "          p.requires_grad_(False)\n",
        "\n",
        "      self.ref = None\n",
        "      if ref_name is not None and kl_coef > 0.0:\n",
        "          self.ref = AutoModelForCausalLM.from_pretrained(ref_name).to(self.device)\n",
        "          for p in self.ref.parameters():\n",
        "              p.requires_grad_(False)\n",
        "\n",
        "      self.rm_tok = AutoTokenizer.from_pretrained(reward_name, use_fast=True)\n",
        "      self.rm = AutoModelForSequenceClassification.from_pretrained(reward_name).to(self.device)\n",
        "      for p in self.rm.parameters():\n",
        "          p.requires_grad_(False)\n",
        "\n",
        "      self.opt = torch.optim.AdamW(self.policy.parameters(), lr=lr)\n",
        "\n",
        "      self.ds = load_dataset(dataset_name, split=split)\n",
        "      self.dl = DataLoader(self.ds, batch_size=batch_size, shuffle=True,\n",
        "                            collate_fn=lambda feats: collate_prompts(feats, prompt_column))\n",
        "\n",
        "  #computes matrix of logprobs\n",
        "  def tokens_logprob_from_forward(self, sequences, T: int) -> torch.Tensor:\n",
        "      \"\"\"\n",
        "      Compute the log-probs of the last T continuation tokens with autograd enabled.\n",
        "      \"\"\"\n",
        "      pad_id = self.tokenizer.pad_token_id\n",
        "      attn = (sequences != pad_id).long()                         # [BxK, L]\n",
        "      outputs = self.policy(sequences, attention_mask=attn)\n",
        "      logits = outputs.logits[:, :-1, :]                          # [BxK, L-1, V]\n",
        "      target = sequences[:, 1:]                                   # [BxK, L-1]\n",
        "\n",
        "      cont_targets = target[:, -T:]                               # [BxK, T]\n",
        "      cont_logits  = logits[:, -T:, :]                            # [BxK, T, V]\n",
        "\n",
        "      logprobs = cont_logits.log_softmax(dim=-1)                  # [BxK, T, V]\n",
        "      chosen = cont_targets.unsqueeze(-1)                         # [BxK, T, 1]\n",
        "      logp = logprobs.gather(-1, chosen).squeeze(-1)   # [BxK, T]\n",
        "      return logp\n",
        "\n",
        "\n",
        "  #performs a single iteration of GRPO\n",
        "  def step(self, prompts: List[str]):\n",
        "    B, K = len(prompts), self.group_size\n",
        "    prompts_rep = [p for p in prompts for _ in range(K)]\n",
        "\n",
        "    # 1. Sync old policy ← current policy (before rollout)\n",
        "    self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "    # 2. Sample rollouts using OLD policy\n",
        "    out, responses, input_len, prompt_lens = generate_with_scores(\n",
        "        self.policy_old, self.tokenizer, prompts_rep,\n",
        "        self.gen_max_new_tokens, self.temperature, self.top_p, self.device\n",
        "    )\n",
        "    sequences = out.sequences                                    # [B*K, L]\n",
        "    T = len(out.scores)                                          # continuation length\n",
        "\n",
        "    # 3. Rewards (scalar per [B,K])\n",
        "    rewards = []\n",
        "    idx = 0\n",
        "    for i in range(B):\n",
        "        row = []\n",
        "        for j in range(K):\n",
        "            r = reward_score(self.rm, self.rm_tok, prompts[i], responses[idx],\n",
        "                             self.rm_max_length, self.device)\n",
        "            row.append(r); idx += 1\n",
        "        rewards.append(row)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)  # [B,K]\n",
        "\n",
        "    # 4. Compute targets (for reuse)\n",
        "    pad_id = self.tokenizer.pad_token_id\n",
        "    attn = (sequences != pad_id).long()\n",
        "    target = sequences[:, 1:]\n",
        "    cont_targets = target[:, -T:]                                # [B*K, T]\n",
        "\n",
        "    # 5. Compute logprobs under OLD policy (fixed, no grad)\n",
        "    with torch.no_grad():\n",
        "        logits_old = self.policy_old(sequences, attention_mask=attn).logits[:, :-1, :]\n",
        "        cont_logits_old = logits_old[:, -T:, :]\n",
        "        logp_old = cont_logits_old.log_softmax(dim=-1)\\\n",
        "                      .gather(-1, cont_targets.unsqueeze(-1))\\\n",
        "                      .squeeze(-1)                               # [B*K, T]\n",
        "\n",
        "    # 6. KL penalty (depends only on sampled sequences)\n",
        "    if self.ref is not None and self.kl_coef > 0.0:\n",
        "        with torch.no_grad():\n",
        "            ref_logits = self.ref(sequences, attention_mask=attn).logits[:, :-1, :]\n",
        "            cont_logits_ref = ref_logits[:, -T:, :]              # [B*K, T, V]\n",
        "            logp_ref = cont_logits_ref.log_softmax(dim=-1)\\\n",
        "                          .gather(-1, cont_targets.unsqueeze(-1))\\\n",
        "                          .squeeze(-1)                           # [B*K, T]\n",
        "        p_ref = logp_ref.exp()\n",
        "        # we’ll recompute p_new inside the μ loop\n",
        "    else:\n",
        "        p_ref = None\n",
        "\n",
        "    # 7. Reshape rewards and advantages\n",
        "    adv = rewards - rewards.mean(dim=1, keepdim=True)            # [B,K]\n",
        "    adv = adv.unsqueeze(-1)                                      # [B,K,1]\n",
        "\n",
        "    # 8. Inner loop: μ GRPO updates\n",
        "    for _ in range(self.mu):\n",
        "        # a) Compute logprobs under NEW policy\n",
        "        logp_new = self.tokens_logprob_from_forward(sequences, T)    # [B*K, T]\n",
        "        logp_new = logp_new.view(B, K, T)\n",
        "        logp_old_ = logp_old.view(B, K, T)\n",
        "        p_ref_ = p_ref.view(B, K, T)\n",
        "\n",
        "        # b) Broadcast advantage\n",
        "        adv_broadcast = adv.expand_as(logp_new)                      # [B,K,T]\n",
        "\n",
        "\n",
        "        # c) PPO clipped objective\n",
        "        ratio   = (logp_new - logp_old_).exp()\n",
        "        clipped = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio)\n",
        "        obj1 = ratio * adv_broadcast\n",
        "        obj2 = clipped * adv_broadcast\n",
        "        pg_loss = -torch.min(obj1, obj2).mean()\n",
        "\n",
        "        # d) KL penalty (recompute p_new each epoch)\n",
        "        if p_ref is not None:\n",
        "            p_new = logp_new.exp()\n",
        "            ratio_kl = p_ref_ / (p_new + 1e-8)\n",
        "            kl_tok = ratio_kl - torch.log(ratio_kl + 1e-8) - 1.0\n",
        "            kl_loss = self.kl_coef * kl_tok.mean()\n",
        "        else:\n",
        "            kl_loss = torch.tensor(0.0, device=self.device)\n",
        "\n",
        "        # e) Total loss\n",
        "        loss = pg_loss + kl_loss\n",
        "\n",
        "        # f) Optimize\n",
        "        self.opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "        self.opt.step()\n",
        "\n",
        "    # After μ epochs, policy is updated (policy_old will sync next step)\n",
        "    return {\n",
        "        \"loss\": float(loss.item()),\n",
        "        \"pg_loss\": float(pg_loss.item()),\n",
        "        \"kl_loss\": float(kl_loss.item()),\n",
        "        \"reward_mean\": float(rewards.mean().item()),\n",
        "    }\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    self.policy.train()\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "        # === Algorithm 1: Step 3 ===\n",
        "        # Update reference model once per epoch\n",
        "        if self.ref is not None:\n",
        "            self.ref.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        for batch in self.dl:\n",
        "            # === Algorithm 1: Step 6 ===\n",
        "            # Update old policy once per batch\n",
        "            self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "            # === Step 7–11 ===\n",
        "            stats = self.step(batch.prompts)\n",
        "\n",
        "            step += 1\n",
        "            if step % 5 == 0:\n",
        "                print(\n",
        "                    f\"epoch {epoch} step {step} | \"\n",
        "                    f\"loss {stats['loss']:.4f} | \"\n",
        "                    f\"pg {stats['pg_loss']:.4f} | \"\n",
        "                    f\"kl {stats['kl_loss']:.4f} | \"\n",
        "                    f\"R {stats['reward_mean']:.3f}\"\n",
        "                )\n",
        "\n",
        "    # Save final model\n",
        "    if self.output_dir:\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        self.policy.save_pretrained(self.output_dir)\n",
        "        self.tokenizer.save_pretrained(self.output_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "l3ctIvdycJag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (adjust model sizes to your GPU):\n",
        "trainer = GRPOTrainer(\n",
        "    dataset_name=\"HuggingFaceH4/ultrachat_200k\",\n",
        "    split=\"train_gen\",\n",
        "    prompt_column=\"prompt\",\n",
        "    policy_name=\"EleutherAI/pythia-70m\",\n",
        "    ref_name=\"EleutherAI/pythia-70m\",  # set None to disable KL\n",
        "    reward_name=\"OpenAssistant/reward-model-deberta-v3-base\",\n",
        "    epochs=1, batch_size=2, group_size=4,\n",
        "    gen_max_new_tokens=64, rm_max_length=512,\n",
        "    temperature=1.0, top_p=0.95, lr=1e-5,\n",
        "    clip_ratio=0.2, kl_coef=1e-5, max_grad_norm=1.0,\n",
        ")\n",
        "trainer.train()\n",
        "print('Saved to', trainer.output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJtU6GA3cPrZ",
        "outputId": "afe05bbb-9820-4714-9c6d-6c17eeb92344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 step 5 | loss -0.2922 | pg -0.2922 | kl 0.0000 | R -1.541\n",
            "epoch 0 step 10 | loss -0.4817 | pg -0.4817 | kl 0.0000 | R -1.038\n",
            "epoch 0 step 15 | loss 0.0000 | pg -0.0000 | kl 0.0000 | R 2.900\n",
            "epoch 0 step 20 | loss -0.2717 | pg -0.2717 | kl 0.0000 | R 0.817\n",
            "epoch 0 step 25 | loss 0.0000 | pg -0.0000 | kl 0.0000 | R 5.850\n",
            "epoch 0 step 30 | loss 0.0000 | pg -0.0000 | kl 0.0000 | R 1.636\n",
            "epoch 0 step 35 | loss 0.0000 | pg -0.0000 | kl 0.0000 | R -1.101\n",
            "epoch 0 step 40 | loss 0.0001 | pg -0.0000 | kl 0.0001 | R 2.779\n",
            "epoch 0 step 45 | loss 0.0001 | pg -0.0000 | kl 0.0001 | R -0.184\n",
            "epoch 0 step 50 | loss 0.0000 | pg -0.0000 | kl 0.0000 | R 0.526\n",
            "epoch 0 step 55 | loss 0.0001 | pg -0.0000 | kl 0.0001 | R 1.532\n",
            "epoch 0 step 60 | loss 0.0001 | pg -0.0000 | kl 0.0001 | R 3.850\n",
            "epoch 0 step 65 | loss 0.0000 | pg -0.0000 | kl 0.0000 | R 1.552\n",
            "epoch 0 step 70 | loss 0.0000 | pg -0.0000 | kl 0.0000 | R -2.325\n",
            "epoch 0 step 75 | loss 0.0000 | pg -0.0000 | kl 0.0000 | R 1.760\n",
            "epoch 0 step 80 | loss 0.0000 | pg -0.0000 | kl 0.0000 | R 0.787\n",
            "epoch 0 step 85 | loss -0.0441 | pg -0.0442 | kl 0.0001 | R 1.058\n",
            "epoch 0 step 90 | loss 0.0000 | pg -0.0000 | kl 0.0000 | R 0.696\n"
          ]
        }
      ]
    }
  ]
}